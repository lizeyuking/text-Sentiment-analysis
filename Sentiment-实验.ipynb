{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 情感分析作业\n",
    "\n",
    "* 说明：本次任务是有关自然语言处理领域中的情感识别问题（也叫观点分析问题），即将用户评价分为正向评价和负向评价两类。本次使用的数据集“online_shopping_10_cats”包含六万多条购物评价，分别来自书籍、平板、手机、水果、洗发水、热水器、蒙牛、衣服、计算机、酒店，共10个类别。\n",
    "* 请按照程序中的提示完成实验。\n",
    "* 第13周：完成数据处理及向量化部分\n",
    "* 第14周：完成LSTM网络部分\n",
    "\n",
    "## 一、数据读入及预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62774, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(62774, 1778)"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence,pack_padded_sequence,pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "path=\"online_shopping_10_cats.csv\"\n",
    "df=pd.read_csv(path)\n",
    "df.head()\n",
    "\n",
    "df=df[[\"review\",\"label\"]]\n",
    "df.head()\n",
    "\n",
    "print(df.shape)\n",
    "df.drop_duplicates()\n",
    "\n",
    "info=re.compile(\"[0-9a-zA-Z]|作者|当当网|京东|洗发水|蒙牛|衣服|酒店|房间\")\n",
    "df[\"review\"]=df[\"review\"].apply(lambda x:info.sub(\"\",str(x)))   #re.sub用于替换字符串中的匹配项\n",
    "df[\"review\"].head() #head( )函数读取前五行数据\n",
    "\n",
    "df[\"words\"]=df[\"review\"].apply(jieba.lcut)\n",
    "df.head()\n",
    "\n",
    "words = []\n",
    "for sentence in df[\"words\"].values:\n",
    "    for word in sentence:\n",
    "        words.append(word)\n",
    "len(words)\n",
    "\n",
    "words = list(set(words))\n",
    "words = sorted(words)\n",
    "len(words)\n",
    "\n",
    "word2idx = {w:i+1 for i,w in enumerate(words)}  #将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标\n",
    "idx2word = {i+1:w for i,w in enumerate(words)}\n",
    "word2idx['<unk>'] = 0\n",
    "idx2word[0] = '<unk>'\n",
    "data = []\n",
    "label = []\n",
    "        \n",
    "for sentence in df['words']:\n",
    "    words_to_idx = []\n",
    "    for word in sentence:\n",
    "        index = word2idx[word]\n",
    "        words_to_idx.append(index)\n",
    "    data.append(words_to_idx)  \n",
    "    #data.append(torch.tensor(words_to_idx))\n",
    "#label = torch.from_numpy(df['label'].values)\n",
    "label = df['label'].values\n",
    "\n",
    "#数据变长处理\n",
    "lenlist=[len(i) for i in data]\n",
    "maxlen=max(lenlist)\n",
    "maxlen\n",
    "\n",
    "data_np=np.zeros((62774,1778))\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(data[i])):\n",
    "        data_np[i][j]=data[i][j]\n",
    "data_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （1）数据读入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>书籍</td>\n",
       "      <td>1</td>\n",
       "      <td>﻿做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>书籍</td>\n",
       "      <td>1</td>\n",
       "      <td>作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>书籍</td>\n",
       "      <td>1</td>\n",
       "      <td>作者长篇大论借用详细报告数据处理工作和计算结果支持其新观点。为什么荷兰曾经县有欧洲最高的生产...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>书籍</td>\n",
       "      <td>1</td>\n",
       "      <td>作者在战几时之前用了＂拥抱＂令人叫绝．日本如果没有战败，就有会有美军的占领，没胡官僚主义的延...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>书籍</td>\n",
       "      <td>1</td>\n",
       "      <td>作者在少年时即喜阅读，能看出他精读了无数经典，因而他有一个庞大的内心世界。他的作品最难能可贵...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cat  label                                             review\n",
       "0  书籍      1  ﻿做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持...\n",
       "1  书籍      1  作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到...\n",
       "2  书籍      1  作者长篇大论借用详细报告数据处理工作和计算结果支持其新观点。为什么荷兰曾经县有欧洲最高的生产...\n",
       "3  书籍      1  作者在战几时之前用了＂拥抱＂令人叫绝．日本如果没有战败，就有会有美军的占领，没胡官僚主义的延...\n",
       "4  书籍      1  作者在少年时即喜阅读，能看出他精读了无数经典，因而他有一个庞大的内心世界。他的作品最难能可贵..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （2）数据筛选及处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>作者长篇大论借用详细报告数据处理工作和计算结果支持其新观点。为什么荷兰曾经县有欧洲最高的生产...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>作者在战几时之前用了＂拥抱＂令人叫绝．日本如果没有战败，就有会有美军的占领，没胡官僚主义的延...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>作者在少年时即喜阅读，能看出他精读了无数经典，因而他有一个庞大的内心世界。他的作品最难能可贵...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  ﻿做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持...      1\n",
       "1  作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到...      1\n",
       "2  作者长篇大论借用详细报告数据处理工作和计算结果支持其新观点。为什么荷兰曾经县有欧洲最高的生产...      1\n",
       "3  作者在战几时之前用了＂拥抱＂令人叫绝．日本如果没有战败，就有会有美军的占领，没胡官僚主义的延...      1\n",
       "4  作者在少年时即喜阅读，能看出他精读了无数经典，因而他有一个庞大的内心世界。他的作品最难能可贵...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （3）去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62774, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>作者长篇大论借用详细报告数据处理工作和计算结果支持其新观点。为什么荷兰曾经县有欧洲最高的生产...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>作者在战几时之前用了＂拥抱＂令人叫绝．日本如果没有战败，就有会有美军的占领，没胡官僚主义的延...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>作者在少年时即喜阅读，能看出他精读了无数经典，因而他有一个庞大的内心世界。他的作品最难能可贵...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>作者有一种专业的谨慎，若能有幸学习原版也许会更好，简体版的书中的印刷错误比较多，影响学者理解...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>作者用诗一样的语言把如水般清澈透明的思想娓娓道来，像一个经验丰富的智慧老人为我们解开一个又一...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>作者提出了一种工作和生活的方式，作为咨询界的元老，不仅能提出理念，而且能够身体力行地实践，并...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>作者妙语连珠，将整个60-70年代用层出不穷的摇滚巨星与自身故事紧紧相连什么是乡愁？什么是摇...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>作者逻辑严密，一气呵成。没有一句废话，深入浅出，循循善诱，环环相扣。让平日里看到指标图释就头...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>作者力从马克思注意经济学角度来剖析当代中国经济细心的人会发现中国近20年来的一些政策措施在何...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>作者结合希尔和卡耐基、汪中求等大师的一些观点，结合中国的实际情况，给渴望成功的青年指出一条实...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>作者更多的是从圆圆母亲的角度来写这个文章。90%的例子都是以圆圆为中心的。我认为，例证不够充...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>作者对于某些电影“表面粗糙”、“内里光滑”的分析，可谓入木三分，为理解那些貌似触及现实而又让...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>作者的理念很好，主要是看一个公司的内部情况来决定股票的情况。操作比较难，作者也承认，所以他说...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>作者的观点独特，语言犀利，深刻的总结了男人与女人之间的是非恩怨，把男人与女人在生活中积累的宿...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>作者的笔触很真实 将一个不忠却又深爱自己妻子的丈夫形象毫无遮掩的展示给读者 而什么是真爱 什...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>作者笔下留情啊，深圳的自由作家远远没有《离婚未遂》笔下那么潇洒。事实上，深圳是一个典型的伪文...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>作者被认为是“爱的奇迹天使”，确实是这样的。很多人认为与孩子的相处一定充满快乐，但却不知一份...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>作为有史以来最伟大的基金经理彼得&amp;#183;林奇凭借其在投资领域杰出的贡献，终其一生的经验和...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>作为一名山西太谷人，从小听多了有关晋商的故事，也去过许多山西的地方，但由于离家已久，故乡渐渐...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>作为一名“白骨精”，我不得不佩服作者入木三分的刻画，不仅精准地描绘出我们的生活画面，更深刻地...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>作为一本职场启示录，不错没有像杜拉拉一样的引入胜的故事，却也用轻松的方式告诉你，如何在职场中...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>作为一本屹立不倒的经典教材，纽摄所传授的不是单纯的技术，而是理念。不是教条的灌输，而是互动性...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>作为一本西方人创作的心理学读物，还是以西方人的视角来看待问题的方式来阐述，目前刚刚开始读，但...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>作为推理小说可能不是最好的，作为言情小说可能也不是最好的。但是结合在一起却很让人惊奇。羡慕那...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>作为女儿6.1的礼物。虽然晚到了几天。等拿到的时候，女儿爱不释手，上洗手间也看，告知不好。竟...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>作为妈妈，我个人很喜欢这本书，但是仍在怀疑这本书是否适合给孩子看？这本书以孩子的视角观看我们...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>昨晚看着看着就睡着了，今天早晨醒来就立马抓起继续啃，正逢小说结尾部分，也正如作者的期望，我被...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>昨天我把这套书看完了，结尾我不是很喜欢有点太戏剧了，但对书中的主人公却是最好的安排，我去过新...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62744</th>\n",
       "      <td>预订时已说明是一家三口人，可旅馆内的设施用品只有两套，要加收费才给第三套，前台的服务员一听说...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62745</th>\n",
       "      <td>这家酒店实在不敢恭维房间差，服务差，就是地方还可以，价格便宜。入住后给的第一间房噪音巨大（前...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62746</th>\n",
       "      <td>我住904浴室没有花洒 喷头还是一根水往外冒 太垃圾了白瞎了我一千多房费 还不如快捷酒店 差差差</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62747</th>\n",
       "      <td>环境较差，马路噪音非常大，而且房间之间的隔音处理得不好，睡在床上能听到楼道里的讲话声。房间偏...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62748</th>\n",
       "      <td>房间稍小，交通不便，专车往返酒店与浦东机场，车程10分钟，但是经常满员，不得不站在车里</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62749</th>\n",
       "      <td>因为到达时间有点晚，所以携程的房间取消了。只好拿了2个套房和4个单间。房间是日式的，面积确实...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62750</th>\n",
       "      <td>一般一般</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62751</th>\n",
       "      <td>四星级酒店，符合标准的地方我就不提了，有以下缺点：1.标间不提供免费瓶装水（吧台内的瓶装水8...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62752</th>\n",
       "      <td>入住时间：2008.5.14地理位置：离景点比较近，离高速公路也比较近，地理位置很好服务态度...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62753</th>\n",
       "      <td>宾馆前台的服务极差，竟然将携程网本人的客房订单弄丢，最终又经历了三次换房经验，惨痛的教训啊！...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62754</th>\n",
       "      <td>出行打车不方便,退房等待时间较长,现在好多酒店都实行免等候退房.酒店楼下的广场舞蛮吵的,周边...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62755</th>\n",
       "      <td>酒店性价比很差！住过无数的酒店，这个算是非常差的！服务差，设施差，房间小，骚扰电话！卫生间淋...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62756</th>\n",
       "      <td>我住的是大床房，房间很小，小到放不下太多行李。不过离地铁站的距离很近，但是由于崇文门地铁最近...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62757</th>\n",
       "      <td>1。 入住时房间有很大的方便面味道 2。 没有小的洗脸毛巾 3。 提供的早餐太差</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62758</th>\n",
       "      <td>岛上没有像样的宾馆，这家马马虎虎过得去。硬件设施很差，没有暖气，最多只能算1+。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62759</th>\n",
       "      <td>非常糟糕的酒店,住了1天,停电两次,酒店装修的材料很劣质,甲醛的味道让人睁不开眼睛.地理位置...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62760</th>\n",
       "      <td>太差,根本达不到5星标准,房间旧装修很久的了,桌子都是坏的,唯一的是宾馆里的环境还可以,标准...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62761</th>\n",
       "      <td>离机场近是唯一的优点 放假没插座是致命的缺点 不会入住第二次</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62762</th>\n",
       "      <td>酒店位置还好，设施太陈旧，太老了！房间面积太小，硬件的设施糟糕！枕头太低！床垫也硬！</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62763</th>\n",
       "      <td>晚上12：30到的，前台不错，打了好多电话问路一直耐心回答，房间也不错挺干净，早上结账的时候...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62764</th>\n",
       "      <td>非常一般的酒店，房里设施很旧，房间送餐竟然要加多50%的送餐费。总之找不到好的方面来说，有其...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62765</th>\n",
       "      <td>房间没窗户，携程网竟然没有说明！</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62766</th>\n",
       "      <td>1、中午快一点到店，说房间没有收拾出来，只好寄存行李出去玩，晚上回到房间满屋烟味，打电话后给...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62767</th>\n",
       "      <td>酒店很旧 房间很小。入住体验非常不好。服务还可以。下面是盘门景区，这点不错。到各景区交通比较方便。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62768</th>\n",
       "      <td>开始以为应该不错，结果大失所望！！顶多跟100块的小旅馆差不多！各种设施都老旧的要命！连淋雨...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62769</th>\n",
       "      <td>我们去盐城的时候那里的最低气温只有4度，晚上冷得要死，居然还不开空调，投诉到酒店客房部，得到...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62770</th>\n",
       "      <td>房间很小，整体设施老化，和四星的差距很大。毛巾太破旧了。早餐很简陋。房间隔音很差，隔两间房间...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62771</th>\n",
       "      <td>我感觉不行。。。性价比很差。不知道是银川都这样还是怎么的！</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62772</th>\n",
       "      <td>房间时间长，进去有点异味！服务员是不是不够用啊！我在一楼找了半个小时以上才找到自己房间，想找...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62773</th>\n",
       "      <td>老人小孩一大家族聚会，选在吴宫泛太平洋，以为新加坡品牌一定很不错，没想到11点30分到前台，...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62743 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  label\n",
       "0      ﻿做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持...      1\n",
       "1      作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到...      1\n",
       "2      作者长篇大论借用详细报告数据处理工作和计算结果支持其新观点。为什么荷兰曾经县有欧洲最高的生产...      1\n",
       "3      作者在战几时之前用了＂拥抱＂令人叫绝．日本如果没有战败，就有会有美军的占领，没胡官僚主义的延...      1\n",
       "4      作者在少年时即喜阅读，能看出他精读了无数经典，因而他有一个庞大的内心世界。他的作品最难能可贵...      1\n",
       "5      作者有一种专业的谨慎，若能有幸学习原版也许会更好，简体版的书中的印刷错误比较多，影响学者理解...      1\n",
       "6      作者用诗一样的语言把如水般清澈透明的思想娓娓道来，像一个经验丰富的智慧老人为我们解开一个又一...      1\n",
       "7      作者提出了一种工作和生活的方式，作为咨询界的元老，不仅能提出理念，而且能够身体力行地实践，并...      1\n",
       "8      作者妙语连珠，将整个60-70年代用层出不穷的摇滚巨星与自身故事紧紧相连什么是乡愁？什么是摇...      1\n",
       "9      作者逻辑严密，一气呵成。没有一句废话，深入浅出，循循善诱，环环相扣。让平日里看到指标图释就头...      1\n",
       "10     作者力从马克思注意经济学角度来剖析当代中国经济细心的人会发现中国近20年来的一些政策措施在何...      1\n",
       "11     作者结合希尔和卡耐基、汪中求等大师的一些观点，结合中国的实际情况，给渴望成功的青年指出一条实...      1\n",
       "12     作者更多的是从圆圆母亲的角度来写这个文章。90%的例子都是以圆圆为中心的。我认为，例证不够充...      1\n",
       "13     作者对于某些电影“表面粗糙”、“内里光滑”的分析，可谓入木三分，为理解那些貌似触及现实而又让...      1\n",
       "14     作者的理念很好，主要是看一个公司的内部情况来决定股票的情况。操作比较难，作者也承认，所以他说...      1\n",
       "15     作者的观点独特，语言犀利，深刻的总结了男人与女人之间的是非恩怨，把男人与女人在生活中积累的宿...      1\n",
       "16     作者的笔触很真实 将一个不忠却又深爱自己妻子的丈夫形象毫无遮掩的展示给读者 而什么是真爱 什...      1\n",
       "17     作者笔下留情啊，深圳的自由作家远远没有《离婚未遂》笔下那么潇洒。事实上，深圳是一个典型的伪文...      1\n",
       "18     作者被认为是“爱的奇迹天使”，确实是这样的。很多人认为与孩子的相处一定充满快乐，但却不知一份...      1\n",
       "19     作为有史以来最伟大的基金经理彼得&#183;林奇凭借其在投资领域杰出的贡献，终其一生的经验和...      1\n",
       "20     作为一名山西太谷人，从小听多了有关晋商的故事，也去过许多山西的地方，但由于离家已久，故乡渐渐...      1\n",
       "21     作为一名“白骨精”，我不得不佩服作者入木三分的刻画，不仅精准地描绘出我们的生活画面，更深刻地...      1\n",
       "22     作为一本职场启示录，不错没有像杜拉拉一样的引入胜的故事，却也用轻松的方式告诉你，如何在职场中...      1\n",
       "23     作为一本屹立不倒的经典教材，纽摄所传授的不是单纯的技术，而是理念。不是教条的灌输，而是互动性...      1\n",
       "24     作为一本西方人创作的心理学读物，还是以西方人的视角来看待问题的方式来阐述，目前刚刚开始读，但...      1\n",
       "25     作为推理小说可能不是最好的，作为言情小说可能也不是最好的。但是结合在一起却很让人惊奇。羡慕那...      1\n",
       "26     作为女儿6.1的礼物。虽然晚到了几天。等拿到的时候，女儿爱不释手，上洗手间也看，告知不好。竟...      1\n",
       "27     作为妈妈，我个人很喜欢这本书，但是仍在怀疑这本书是否适合给孩子看？这本书以孩子的视角观看我们...      1\n",
       "28     昨晚看着看着就睡着了，今天早晨醒来就立马抓起继续啃，正逢小说结尾部分，也正如作者的期望，我被...      1\n",
       "29     昨天我把这套书看完了，结尾我不是很喜欢有点太戏剧了，但对书中的主人公却是最好的安排，我去过新...      1\n",
       "...                                                  ...    ...\n",
       "62744  预订时已说明是一家三口人，可旅馆内的设施用品只有两套，要加收费才给第三套，前台的服务员一听说...      0\n",
       "62745  这家酒店实在不敢恭维房间差，服务差，就是地方还可以，价格便宜。入住后给的第一间房噪音巨大（前...      0\n",
       "62746   我住904浴室没有花洒 喷头还是一根水往外冒 太垃圾了白瞎了我一千多房费 还不如快捷酒店 差差差      0\n",
       "62747  环境较差，马路噪音非常大，而且房间之间的隔音处理得不好，睡在床上能听到楼道里的讲话声。房间偏...      0\n",
       "62748        房间稍小，交通不便，专车往返酒店与浦东机场，车程10分钟，但是经常满员，不得不站在车里      0\n",
       "62749  因为到达时间有点晚，所以携程的房间取消了。只好拿了2个套房和4个单间。房间是日式的，面积确实...      0\n",
       "62750                                               一般一般      0\n",
       "62751  四星级酒店，符合标准的地方我就不提了，有以下缺点：1.标间不提供免费瓶装水（吧台内的瓶装水8...      0\n",
       "62752  入住时间：2008.5.14地理位置：离景点比较近，离高速公路也比较近，地理位置很好服务态度...      0\n",
       "62753  宾馆前台的服务极差，竟然将携程网本人的客房订单弄丢，最终又经历了三次换房经验，惨痛的教训啊！...      0\n",
       "62754  出行打车不方便,退房等待时间较长,现在好多酒店都实行免等候退房.酒店楼下的广场舞蛮吵的,周边...      0\n",
       "62755  酒店性价比很差！住过无数的酒店，这个算是非常差的！服务差，设施差，房间小，骚扰电话！卫生间淋...      0\n",
       "62756  我住的是大床房，房间很小，小到放不下太多行李。不过离地铁站的距离很近，但是由于崇文门地铁最近...      0\n",
       "62757           1。 入住时房间有很大的方便面味道 2。 没有小的洗脸毛巾 3。 提供的早餐太差      0\n",
       "62758           岛上没有像样的宾馆，这家马马虎虎过得去。硬件设施很差，没有暖气，最多只能算1+。      0\n",
       "62759  非常糟糕的酒店,住了1天,停电两次,酒店装修的材料很劣质,甲醛的味道让人睁不开眼睛.地理位置...      0\n",
       "62760  太差,根本达不到5星标准,房间旧装修很久的了,桌子都是坏的,唯一的是宾馆里的环境还可以,标准...      0\n",
       "62761                     离机场近是唯一的优点 放假没插座是致命的缺点 不会入住第二次      0\n",
       "62762         酒店位置还好，设施太陈旧，太老了！房间面积太小，硬件的设施糟糕！枕头太低！床垫也硬！      0\n",
       "62763  晚上12：30到的，前台不错，打了好多电话问路一直耐心回答，房间也不错挺干净，早上结账的时候...      0\n",
       "62764  非常一般的酒店，房里设施很旧，房间送餐竟然要加多50%的送餐费。总之找不到好的方面来说，有其...      0\n",
       "62765                                   房间没窗户，携程网竟然没有说明！      0\n",
       "62766  1、中午快一点到店，说房间没有收拾出来，只好寄存行李出去玩，晚上回到房间满屋烟味，打电话后给...      0\n",
       "62767  酒店很旧 房间很小。入住体验非常不好。服务还可以。下面是盘门景区，这点不错。到各景区交通比较方便。      0\n",
       "62768  开始以为应该不错，结果大失所望！！顶多跟100块的小旅馆差不多！各种设施都老旧的要命！连淋雨...      0\n",
       "62769  我们去盐城的时候那里的最低气温只有4度，晚上冷得要死，居然还不开空调，投诉到酒店客房部，得到...      0\n",
       "62770  房间很小，整体设施老化，和四星的差距很大。毛巾太破旧了。早餐很简陋。房间隔音很差，隔两间房间...      0\n",
       "62771                      我感觉不行。。。性价比很差。不知道是银川都这样还是怎么的！      0\n",
       "62772  房间时间长，进去有点异味！服务员是不是不够用啊！我在一楼找了半个小时以上才找到自己房间，想找...      0\n",
       "62773  老人小孩一大家族聚会，选在吴宫泛太平洋，以为新加坡品牌一定很不错，没想到11点30分到前台，...      0\n",
       "\n",
       "[62743 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （4）数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ﻿做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持...\n",
       "1    真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到真理...\n",
       "2    长篇大论借用详细报告数据处理工作和计算结果支持其新观点。为什么荷兰曾经县有欧洲最高的生产率？...\n",
       "3    在战几时之前用了＂拥抱＂令人叫绝．日本如果没有战败，就有会有美军的占领，没胡官僚主义的延续，...\n",
       "4    在少年时即喜阅读，能看出他精读了无数经典，因而他有一个庞大的内心世界。他的作品最难能可贵的有...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （5）分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\王军懿\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.980 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持...</td>\n",
       "      <td>1</td>\n",
       "      <td>[﻿, 做, 父母, 一定, 要, 有, 刘墉, 这样, 的, 心态, ，, 不断, 地, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到真理...</td>\n",
       "      <td>1</td>\n",
       "      <td>[真有, 英国人, 严谨, 的, 风格, ，, 提出, 观点, 、, 进行, 论述, 论证,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>长篇大论借用详细报告数据处理工作和计算结果支持其新观点。为什么荷兰曾经县有欧洲最高的生产率？...</td>\n",
       "      <td>1</td>\n",
       "      <td>[长篇大论, 借用, 详细, 报告, 数据处理, 工作, 和, 计算结果, 支持, 其新, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>在战几时之前用了＂拥抱＂令人叫绝．日本如果没有战败，就有会有美军的占领，没胡官僚主义的延续，...</td>\n",
       "      <td>1</td>\n",
       "      <td>[在, 战, 几时, 之前, 用, 了, ＂, 拥抱, ＂, 令人, 叫绝, ．, 日本, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>在少年时即喜阅读，能看出他精读了无数经典，因而他有一个庞大的内心世界。他的作品最难能可贵的有...</td>\n",
       "      <td>1</td>\n",
       "      <td>[在, 少年, 时即, 喜, 阅读, ，, 能, 看出, 他, 精读, 了, 无数, 经典,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label  \\\n",
       "0  ﻿做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持...      1   \n",
       "1  真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到真理...      1   \n",
       "2  长篇大论借用详细报告数据处理工作和计算结果支持其新观点。为什么荷兰曾经县有欧洲最高的生产率？...      1   \n",
       "3  在战几时之前用了＂拥抱＂令人叫绝．日本如果没有战败，就有会有美军的占领，没胡官僚主义的延续，...      1   \n",
       "4  在少年时即喜阅读，能看出他精读了无数经典，因而他有一个庞大的内心世界。他的作品最难能可贵的有...      1   \n",
       "\n",
       "                                               words  \n",
       "0  [﻿, 做, 父母, 一定, 要, 有, 刘墉, 这样, 的, 心态, ，, 不断, 地, ...  \n",
       "1  [真有, 英国人, 严谨, 的, 风格, ，, 提出, 观点, 、, 进行, 论述, 论证,...  \n",
       "2  [长篇大论, 借用, 详细, 报告, 数据处理, 工作, 和, 计算结果, 支持, 其新, ...  \n",
       "3  [在, 战, 几时, 之前, 用, 了, ＂, 拥抱, ＂, 令人, 叫绝, ．, 日本, ...  \n",
       "4  [在, 少年, 时即, 喜, 阅读, ，, 能, 看出, 他, 精读, 了, 无数, 经典,...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （6）建立词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2277845"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64104"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （7）将中文词数字化表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "62774"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "62774"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1778"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62774, 1778)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （8）划分训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train,x_val,y_train,y_val=train_test_split(data_np,label,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （9）设置DataSet和DataLoader\n",
    "提供现成的数据变长处理的方法，可以直接在DataLoader的参数中设置collate_fn=mycollate_fn来使用这个方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class mDataSet(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.len = len(x)\n",
    "        self.x_data = torch.from_numpy(x)\n",
    "        self.y_data = torch.from_numpy(y)\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "       \n",
    " \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b=64\n",
    "#设置用于验证的DataSet\n",
    "trainDataset=mDataSet(x_train,y_train)\n",
    "#设置用于训练的DataLoader\n",
    "train_loader = DataLoader(dataset=trainDataset,   # 传递数据集\n",
    "                          batch_size=b,     # 小批量的数据大小，每次加载一batch数据\n",
    "                          shuffle=True,      # 打乱数据之间的顺序\n",
    "                          )\n",
    "#设置用于验证的DataSet\n",
    "validateDataset=mDataSet(x_val,y_val)\n",
    "#设置用于验证的DataLoader\n",
    "validate_loader=DataLoader(dataset=validateDataset,   # 传递数据集\n",
    "                          batch_size=b,     # 小批量的数据大小，每次加载一batch数据\n",
    "                          shuffle=False,      # 打乱数据之间的顺序\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、建立模型\n",
    "### （1）定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(words), 50) #batch*maxlen*50\n",
    "        self.num_layers=3\n",
    "        self.hidden_size=100\n",
    "        self.lstm = nn.LSTM(input_size=50, hidden_size=100, num_layers=3,\n",
    "                                   batch_first=True)\n",
    "        self.dropout=nn.Dropout(p=0.5, inplace=False)\n",
    "        self.fc1 = nn.Linear(in_features=100, out_features=256, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=32, bias=True)\n",
    "        self.fc3 = nn.Linear(in_features=32, out_features=2, bias=True)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "    def forward(self, input):\n",
    "        x = self.embedding(input)  # [batch_size, max_len, 100]\n",
    "        #x = pack_padded_sequence(x,maxlen, batch_first=True)\n",
    "        h0 = Variable(torch.zeros(self.num_layers , x.size(0), self.hidden_size)).cuda()\n",
    "        c0 = Variable(torch.zeros(self.num_layers , x.size(0), self.hidden_size)).cuda()\n",
    "        out, dd = self.lstm(x, (h0, c0))\n",
    "        #x, (h_n, c_n) = self.lstm(x)\n",
    "        #output_fw = h_n[-2, :, :]  # 正向最后一次的输出\n",
    "        #output_bw = h_n[-1, :, :]  # 反向最后一次的输出\n",
    "        #output = torch.cat([output_fw, output_bw], dim=-1) \n",
    "       # print(out.shape)\n",
    "        out = out[:,-1,:].squeeze()\n",
    "        #out=out.flatten(1)\n",
    "        out = self.dropout(torch.tanh(self.fc1(out)))\n",
    "        out = torch.tanh(self.fc2(out))  \n",
    "        #print(out.shape)\n",
    "        out = self.sigmoid(self.fc3(out))\n",
    "        #print(out)\n",
    "        # 可以考虑再添加一个全连接层作为输出层，激活函数处理。\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Model(\n  (embedding): Embedding(64104, 50)\n  (lstm): LSTM(50, 100, num_layers=3, batch_first=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc1): Linear(in_features=100, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=32, bias=True)\n  (fc3): Linear(in_features=32, out_features=2, bias=True)\n  (sigmoid): Sigmoid()\n)"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=Model().cuda()\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （2）初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 实例化模型\n",
    "#model=Model().cuda()\n",
    "# 定义优化器\n",
    "optimizer=torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "# 学习率调整（可选）\n",
    "\n",
    "# 定义损失函数\n",
    "\n",
    "lossfunc=nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Model(\n  (embedding): Embedding(64104, 50)\n  (lstm): LSTM(50, 100, num_layers=3, batch_first=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc1): Linear(in_features=100, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=32, bias=True)\n  (fc3): Linear(in_features=32, out_features=2, bias=True)\n  (sigmoid): Sigmoid()\n)"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#可供参考的模型结构\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （3）准确率指标\n",
    "可用于参考的准确率指标计算方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AvgrageMeter(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.cnt = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.sum += val * n\n",
    "        self.cnt += n\n",
    "        self.avg = self.sum / self.cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(output, label, topk=(1,)):\n",
    "    maxk = max(topk) \n",
    "    batch_size = label.size(0)\n",
    "\n",
    "    # 获取前K的索引\n",
    "    _, pred = output.topk(maxk, 1, True, True) #使用topk来获得前k个的索引\n",
    "    pred = pred.t() # 进行转置\n",
    "    # eq按照对应元素进行比较 view(1,-1) 自动转换到行为1,的形状， expand_as(pred) 扩展到pred的shape\n",
    "    # expand_as 执行按行复制来扩展，要保证列相等\n",
    "    correct = pred.eq(label.view(1, -1).expand_as(pred)) # 与正确标签序列形成的矩阵相比，生成True/False矩阵\n",
    "#     print(correct)\n",
    "\n",
    "    rtn = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0) # 前k行的数据 然后平整到1维度，来计算true的总个数\n",
    "        rtn.append(correct_k.mul_(100.0 / batch_size)) # mul_() ternsor 的乘法  正确的数目/总的数目 乘以100 变成百分比\n",
    "    return rtn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （4）训练 + 验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs=100\n",
    "def train(epoch,epochs, train_loader, model, optimizer,lossfunc):\n",
    "    model.train()\n",
    "    \n",
    "    #data_loader = get_dataloader(True)\n",
    "    for idx, (data,target) in enumerate(train_loader):\n",
    "        data=data.long()\n",
    "        data = Variable(torch.LongTensor(data)).cuda()\n",
    "        target=target.long()\n",
    "        target = Variable(torch.tensor(target)).cuda()\n",
    "       \n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "        output = model(data)\n",
    "        #print(output.shape)\n",
    "        #print(target.shape)\n",
    "        loss = lossfunc(output,target)\n",
    "           \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # 梯度更新\n",
    "        optimizer.step()\n",
    "       # print(epoch,idx,loss.item())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\17801\\pycharmprojects\\nlpacademic\\venv\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-79-209caab55751>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m     \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepoch\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mlossfunc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m     \u001B[1;31m#if((epoch+1)%10==0):\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[1;31m#print(epoch,loss.item())\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-78-ceea4396395a>\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(epoch, epochs, train_loader, model, optimizer, lossfunc)\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m         \u001B[1;31m# 反向传播\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 22\u001B[1;33m         \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     23\u001B[0m         \u001B[1;31m# 梯度更新\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\17801\\pycharmprojects\\nlpacademic\\venv\\lib\\site-packages\\torch\\_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    253\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    254\u001B[0m         \u001B[0mThe\u001B[0m \u001B[0mgraph\u001B[0m \u001B[1;32mis\u001B[0m \u001B[0mdifferentiated\u001B[0m \u001B[0musing\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mchain\u001B[0m \u001B[0mrule\u001B[0m\u001B[1;33m.\u001B[0m \u001B[0mIf\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mtensor\u001B[0m \u001B[1;32mis\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 255\u001B[1;33m         \u001B[0mnon\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mscalar\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m \u001B[0mits\u001B[0m \u001B[0mdata\u001B[0m \u001B[0mhas\u001B[0m \u001B[0mmore\u001B[0m \u001B[0mthan\u001B[0m \u001B[0mone\u001B[0m \u001B[0melement\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mrequires\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    256\u001B[0m         \u001B[0mgradient\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mfunction\u001B[0m \u001B[0madditionally\u001B[0m \u001B[0mrequires\u001B[0m \u001B[0mspecifying\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mgradient\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    257\u001B[0m         \u001B[0mIt\u001B[0m \u001B[0mshould\u001B[0m \u001B[0mbe\u001B[0m \u001B[0ma\u001B[0m \u001B[0mtensor\u001B[0m \u001B[0mof\u001B[0m \u001B[0mmatching\u001B[0m \u001B[0mtype\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mlocation\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mthat\u001B[0m \u001B[0mcontains\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\17801\\pycharmprojects\\nlpacademic\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    147\u001B[0m         \u001B[0mtuple\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0minputs\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mtuple\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    148\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 149\u001B[1;33m     \u001B[0mgrad_tensors_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_tensor_or_tensors_to_tuple\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrad_tensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensors\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    150\u001B[0m     \u001B[0mgrad_tensors_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_make_grads\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    151\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mretain_graph\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train(epoch,epochs, train_loader, model, optimizer,lossfunc)\n",
    "    #if((epoch+1)%10==0):\n",
    "    print(epoch,loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(epoch,validate_loader, device, model, criterion, tensorboard_path):\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练集上的loss约为0.39，准确率83%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62774, 2)\n",
      "2842\n",
      "55.546420492560614\n"
     ]
    },
    {
     "data": {
      "text/plain": "1778"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence,pack_padded_sequence,pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "path=\"online_shopping_10_cats.csv\"\n",
    "df=pd.read_csv(path)\n",
    "df.head()\n",
    "\n",
    "df=df[[\"review\",\"label\"]]\n",
    "df.head()\n",
    "\n",
    "print(df.shape)\n",
    "df.drop_duplicates()\n",
    "\n",
    "info=re.compile(\"[0-9a-zA-Z]|作者|当当网|京东|洗发水|蒙牛|衣服|酒店|房间\")\n",
    "df[\"review\"]=df[\"review\"].apply(lambda x:info.sub(\"\",str(x)))   #re.sub用于替换字符串中的匹配项\n",
    "df[\"review\"].head() #head( )函数读取前五行数据\n",
    "\n",
    "df[\"words\"]=df[\"review\"].apply(jieba.lcut)\n",
    "df.head()\n",
    "\n",
    "words = []\n",
    "for sentence in df[\"words\"].values:\n",
    "    for word in sentence:\n",
    "        words.append(word)\n",
    "len(words)\n",
    "\n",
    "words = list(set(words))\n",
    "words = sorted(words)\n",
    "len(words)\n",
    "#embedding lookup要求输入的网络数据是整数。最简单的方法就是创建数据字典：{单词：整数}。然后将评论全部一一对应转换成整数，传入网络。\n",
    "\n",
    "word2idx = {w:i+1 for i,w in enumerate(words)}\n",
    "idx2word = {i+1:w for i,w in enumerate(words)}\n",
    "word2idx['<unk>'] = 0\n",
    "idx2word[0] = '<unk>'\n",
    "data = []\n",
    "label = []\n",
    "\n",
    "for sentence in df['words']:\n",
    "    words_to_idx = []\n",
    "    for word in sentence:\n",
    "        index = word2idx[word]\n",
    "        words_to_idx.append(index)\n",
    "    data.append(words_to_idx)\n",
    "    #data.append(torch.tensor(words_to_idx))\n",
    "#label = torch.from_numpy(df['label'].values)\n",
    "label = df['label'].values\n",
    "print(np.max([len(x) for x in df[\"review\"]]))\n",
    "print(np.mean([len(x) for x in df[\"review\"]]))\n",
    "#数据变长处理\n",
    "lenlist=[len(i) for i in data]\n",
    "maxlen=max(lenlist)\n",
    "maxlen\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "data_np=np.zeros((62774,50))\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(data[i])):\n",
    "        data_np[i][j]=data[i][j]\n",
    "data_np.shape\n",
    "x_train,x_val,y_train,y_val=train_test_split(data_np,label,test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "\n",
    "class mDataSet(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.len = len(x)\n",
    "        self.x_data = torch.from_numpy(x)\n",
    "        self.y_data = torch.from_numpy(y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "b=64\n",
    "#设置用于验证的DataSet\n",
    "trainDataset=mDataSet(x_train,y_train)\n",
    "#设置用于训练的DataLoader\n",
    "train_loader = DataLoader(dataset=trainDataset,   # 传递数据集\n",
    "                          batch_size=b,     # 小批量的数据大小，每次加载一batch数据\n",
    "                          shuffle=True,      # 打乱数据之间的顺序\n",
    "                          )\n",
    "#设置用于验证的DataSet\n",
    "validateDataset=mDataSet(x_val,y_val)\n",
    "#设置用于验证的DataLoader\n",
    "validate_loader=DataLoader(dataset=validateDataset,   # 传递数据集\n",
    "                          batch_size=b,     # 小批量的数据大小，每次加载一batch数据\n",
    "                          shuffle=False,      # 打乱数据之间的顺序\n",
    "                          )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(words)+1, 50) #batch*maxlen*50\n",
    "        self.num_layers=3\n",
    "        self.hidden_size=100\n",
    "        self.lstm = nn.LSTM(input_size=50, hidden_size=100, num_layers=3,\n",
    "                                   batch_first=True)\n",
    "        self.dropout=nn.Dropout(p=0.5, inplace=False)\n",
    "        self.fc1 = nn.Linear(in_features=100, out_features=256, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=32, bias=True)\n",
    "        self.fc3 = nn.Linear(in_features=32, out_features=2, bias=True)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "    def forward(self, input):\n",
    "        x = self.embedding(input)  # [batch_size, max_len, 100]\n",
    "        #x = pack_padded_sequence(x,maxlen, batch_first=True)\n",
    "        h0 = Variable(torch.zeros(self.num_layers , x.size(0), self.hidden_size))\n",
    "        c0 = Variable(torch.zeros(self.num_layers , x.size(0), self.hidden_size))\n",
    "        out, dd = self.lstm(x, (h0, c0))\n",
    "        #x, (h_n, c_n) = self.lstm(x)\n",
    "        #output_fw = h_n[-2, :, :]  # 正向最后一次的输出\n",
    "        #output_bw = h_n[-1, :, :]  # 反向最后一次的输出\n",
    "        #output = torch.cat([output_fw, output_bw], dim=-1)\n",
    "       # print(out.shape)\n",
    "        out = out[:,-1,:].squeeze()\n",
    "        #out=out.flatten(1)\n",
    "        out = self.dropout(torch.tanh(self.fc1(out)))\n",
    "        out = torch.tanh(self.fc2(out))\n",
    "        #print(out.shape)\n",
    "        out = self.sigmoid(self.fc3(out))\n",
    "        #print(out)\n",
    "        # 可以考虑再添加一个全连接层作为输出层，激活函数处理。\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "Model(\n  (embedding): Embedding(64105, 1778)\n  (lstm): LSTM(1778, 100, num_layers=3, batch_first=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc1): Linear(in_features=100, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=32, bias=True)\n  (fc3): Linear(in_features=32, out_features=2, bias=True)\n  (sigmoid): Sigmoid()\n)"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=Model()\n",
    "\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-27-f2b15ae7bb58>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0midx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtarget\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m     \u001B[0mdata\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m     \u001B[1;32mbreak\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "for idx, (data,target) in enumerate(train_loader):\n",
    "    data=data.to(device)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "# 实例化模型\n",
    "#model=Model().cuda()\n",
    "# 定义优化器\n",
    "optimizer=torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "# 学习率调整（可选）\n",
    "\n",
    "# 定义损失函数\n",
    "\n",
    "lossfunc=nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "class AvgrageMeter(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.cnt = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.sum += val * n\n",
    "        self.cnt += n\n",
    "        self.avg = self.sum / self.cnt\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "def accuracy(output, label, topk=(1,)):\n",
    "    maxk = max(topk)\n",
    "    batch_size = label.size(0)\n",
    "\n",
    "    # 获取前K的索引\n",
    "    _, pred = output.topk(maxk, 1, True, True) #使用topk来获得前k个的索引\n",
    "    pred = pred.t() # 进行转置\n",
    "    # eq按照对应元素进行比较 view(1,-1) 自动转换到行为1,的形状， expand_as(pred) 扩展到pred的shape\n",
    "    # expand_as 执行按行复制来扩展，要保证列相等\n",
    "    correct = pred.eq(label.view(1, -1).expand_as(pred)) # 与正确标签序列形成的矩阵相比，生成True/False矩阵\n",
    "#     print(correct)\n",
    "\n",
    "    rtn = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0) # 前k行的数据 然后平整到1维度，来计算true的总个数\n",
    "        rtn.append(correct_k.mul_(100.0 / batch_size)) # mul_() ternsor 的乘法  正确的数目/总的数目 乘以100 变成百分比\n",
    "    return rtn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "epochs=1\n",
    "def train(epoch,epochs, train_loader, model, optimizer,lossfunc):\n",
    "    model.train()\n",
    "    #data_loader = get_dataloader(True)\n",
    "    for idx, (data,target) in enumerate(train_loader):\n",
    "        data=data.long()\n",
    "        data = Variable(torch.LongTensor(data))\n",
    "        target=target.long()\n",
    "        target = Variable(torch.tensor(target))\n",
    "\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        output = model(data)\n",
    "        #print(output.shape)\n",
    "        #print(target.shape)\n",
    "        loss = lossfunc(output,target)\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # 梯度更新\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "       # print(epoch,idx,loss.item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\17801\\pycharmprojects\\nlpacademic\\venv\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-63-c57edab73a69>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m     \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mlossfunc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m     \u001B[1;32mbreak\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[1;31m#if((epoch+1)%10==0):\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepoch\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-61-2ec9e62ed3ca>\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(epoch, epochs, train_loader, model, optimizer, lossfunc)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m         \u001B[1;31m# 反向传播\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 21\u001B[1;33m         \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     22\u001B[0m         \u001B[1;31m# 梯度更新\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     23\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\17801\\pycharmprojects\\nlpacademic\\venv\\lib\\site-packages\\torch\\_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    253\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    254\u001B[0m         \u001B[0mThe\u001B[0m \u001B[0mgraph\u001B[0m \u001B[1;32mis\u001B[0m \u001B[0mdifferentiated\u001B[0m \u001B[0musing\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mchain\u001B[0m \u001B[0mrule\u001B[0m\u001B[1;33m.\u001B[0m \u001B[0mIf\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mtensor\u001B[0m \u001B[1;32mis\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 255\u001B[1;33m         \u001B[0mnon\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mscalar\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m \u001B[0mits\u001B[0m \u001B[0mdata\u001B[0m \u001B[0mhas\u001B[0m \u001B[0mmore\u001B[0m \u001B[0mthan\u001B[0m \u001B[0mone\u001B[0m \u001B[0melement\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mrequires\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    256\u001B[0m         \u001B[0mgradient\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mfunction\u001B[0m \u001B[0madditionally\u001B[0m \u001B[0mrequires\u001B[0m \u001B[0mspecifying\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mgradient\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    257\u001B[0m         \u001B[0mIt\u001B[0m \u001B[0mshould\u001B[0m \u001B[0mbe\u001B[0m \u001B[0ma\u001B[0m \u001B[0mtensor\u001B[0m \u001B[0mof\u001B[0m \u001B[0mmatching\u001B[0m \u001B[0mtype\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mlocation\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mthat\u001B[0m \u001B[0mcontains\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\17801\\pycharmprojects\\nlpacademic\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    147\u001B[0m         \u001B[0mtuple\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0minputs\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mtuple\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    148\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 149\u001B[1;33m     \u001B[0mgrad_tensors_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_tensor_or_tensors_to_tuple\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrad_tensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensors\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    150\u001B[0m     \u001B[0mgrad_tensors_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_make_grads\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    151\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mretain_graph\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(epochs):\n",
    "    train(epoch, epochs, train_loader, model, optimizer,lossfunc)\n",
    "    break\n",
    "    #if((epoch+1)%10==0):\n",
    "    print(epoch,loss.item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}